{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6480e9ea",
   "metadata": {},
   "source": [
    "# Jared Godar Regression Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73f641c",
   "metadata": {},
   "source": [
    "This is the overall working notebook used to acquire / prepare / clean / scale / and explore my zillo data.\n",
    "\n",
    "The modeling and evaluation portion will be in a second notebook `zillo-modeling.ipynb`\n",
    "\n",
    "Streamlined highlights from both notebooks can be found in the `zillo-report.ipynb` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e1dca4",
   "metadata": {},
   "source": [
    "Import libraries used in project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e25a03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "#Vizualization Tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Modeling Tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from datetime import date\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "## Evaluation tools\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from math import sqrt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Custim functions\n",
    "from env import host, user, password #Database credentials\n",
    "import zillo_wrangle\n",
    "import z_wrangle2\n",
    "import z_wrangle3\n",
    "import eval_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ceaa0d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aff8bb",
   "metadata": {},
   "source": [
    "## Acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c61d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to contact database\n",
    "def get_db_url(db_name):\n",
    "    return f\"mysql+pymysql://{user}:{password}@{host}/{db_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f1fa37",
   "metadata": {},
   "source": [
    "- Look at zillow data dictionary. \n",
    "- Import minimum features (beds, bath, tax, year, fips)\n",
    "- See what other columns may prove useful in model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34765b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.read_excel('zillow_data_dictionary.xlsx')\n",
    "dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28902634",
   "metadata": {},
   "source": [
    "### Takeaways: Columns of interest that may have predictive value\n",
    "\n",
    "- `buildingqualitytypeid` Quality \n",
    "- `fireplacecnt`\n",
    "- `garagecarcnt`\n",
    "- `poolcnt`\n",
    "- `rawcensustractandblock`\n",
    "- `censustractandblock`\n",
    "- `regionidzip`\n",
    "- `regionidneighborhood`\n",
    "- `storytypeid`\n",
    "\n",
    "\n",
    "import:\n",
    "- `regionidcounty`\n",
    "- \n",
    "- `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230e0600",
   "metadata": {},
   "source": [
    "Use SQL query to get single unit (`propertylandusetypeid=261`) from May-Aug, 2017 filtering for non-zero values to have fewer nulls in the first data pull to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0ba884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_sql():\n",
    "    query = \"\"\"\n",
    "    SELECT bedroomcnt as bedrooms, \n",
    "       bathroomcnt as bathrooms,\n",
    "       calculatedfinishedsquarefeet as square_feet,\n",
    "       yearbuilt as year,\n",
    "       taxamount as taxes,\n",
    "       taxvaluedollarcnt as home_value,\n",
    "       fips as fips,\n",
    "       regionidzip as zip_code\n",
    "    FROM predictions_2017\n",
    "     JOIN properties_2017 USING(parcelid)\n",
    "    WHERE (transactiondate >= '2017-01-01' AND transactiondate <= '2017-12-31') \n",
    "        AND propertylandusetypeid = '261'\n",
    "        AND bedroomcnt > 0\n",
    "        AND bathroomcnt > 0\n",
    "        AND calculatedfinishedsquarefeet > 0 \n",
    "        AND taxamount > 0\n",
    "        AND taxvaluedollarcnt > 0\n",
    "        AND fips > 0\n",
    "    ORDER BY fips;\n",
    "    \"\"\"\n",
    "    df = pd.read_sql(query, get_db_url('zillow'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0398b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow = get_data_from_sql()\n",
    "zillow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de01544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape1 = zillow.shape\n",
    "shape1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5142faf",
   "metadata": {},
   "source": [
    "Count nulls by column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa3a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.info(null_counts=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa5ced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total of null values for each row\n",
    "null1 = zillow.isnull().sum()\n",
    "null1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c685af47",
   "metadata": {},
   "source": [
    "Lots of missing neighborhood data... Drop that column before filtering NAs. (Actually removed this field from the SQL query - now not imported and not dropped)\n",
    "\n",
    "- [ ] Drop city as well\n",
    "- [ ] Figure out how to get that information from `fips`  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b008706",
   "metadata": {},
   "source": [
    "GO back to mysql workbench and see how many properties have the `single residential inferred` code 279\n",
    "\n",
    "- 55614 for single family\n",
    "\n",
    "- 0 records for inferred single family, so no need to include it in query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c89b17a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1002d5",
   "metadata": {},
   "source": [
    "### Vizualize distribution and outliers\n",
    "\n",
    "- Eliminating outliers may also reduce the null value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b782e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 3))\n",
    "\n",
    "# List of columns\n",
    "cols = [col for col in zillow.columns if col not in ['fips', 'year_built', 'zip_code', 'propertylandusedesc']]\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "\n",
    "    # i starts at 0, but plot nos should start at 1\n",
    "    plot_number = i + 1 \n",
    "\n",
    "    # Create subplot.\n",
    "    plt.subplot(1, len(cols), plot_number)\n",
    "\n",
    "    # Title with column name.\n",
    "    plt.title(col)\n",
    "\n",
    "    # Display histogram for column.\n",
    "    zillow[col].hist(bins=5)\n",
    "\n",
    "    # Hide gridlines.\n",
    "    plt.grid(False)\n",
    "    \n",
    "    # turn off scientific notation\n",
    "    #plt.ticklabel_format(useOffset=False)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02204a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "sns.boxplot(data=zillow.drop(columns=['fips']))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8431d17",
   "metadata": {},
   "source": [
    "Lots of outliers - especially in value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5175d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.describe().apply(lambda s: s.apply(lambda x: format(x, 'g')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa3e70d",
   "metadata": {},
   "source": [
    "### Remove outliers\n",
    "\n",
    "Make remove outliers function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23898e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, k, col_list):\n",
    "    ''' remove outliers specified columns in a dataframe given a user-enterd cutoff value\n",
    "    '''\n",
    "    \n",
    "    for col in col_list:\n",
    "\n",
    "        q1, q3 = df[col].quantile([.25, .75])  # get quartiles\n",
    "        \n",
    "        iqr = q3 - q1   # calculate interquartile range\n",
    "        \n",
    "        upper_bound = q3 + k * iqr   # get upper bound\n",
    "        lower_bound = q1 - k * iqr   # get lower bound\n",
    "\n",
    "        # return dataframe without outliers\n",
    "        \n",
    "        df = df[(df[col] > lower_bound) & (df[col] < upper_bound)]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ad1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b421ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow = remove_outliers(zillow, 1.5, ['bedrooms', 'bathrooms', 'square_feet', 'taxes', 'home_value'])\n",
    "zillow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496bb9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape2 = zillow.shape\n",
    "print(shape1)\n",
    "print(shape2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146aa4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed1 = shape1[0]-shape2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6caa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Original records: {shape1[0]}')\n",
    "print(f'Records Removed: {removed1}')\n",
    "print(f'Records remaining: {shape2[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d53fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total of null values for each row\n",
    "null2 = zillow.isnull().sum()\n",
    "print(null1)\n",
    "print(null2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373b7a91",
   "metadata": {},
   "source": [
    "Reasonable number of null values copared to total records, go ahead and drop NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbce735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NAs\n",
    "zillow = zillow.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92bb668",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape3=zillow.shape\n",
    "shape3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4655d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed2=shape2[0]-shape3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33bb7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Original records: {shape2[0]}')\n",
    "print(f'Records Removed: {removed2}')\n",
    "print(f'Records remaining: {shape3[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82cd527",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676866b0",
   "metadata": {},
   "source": [
    "### Vizualize distributions again minus outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d205c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 3))\n",
    "\n",
    "# List of columns\n",
    "cols = [col for col in zillow.columns if col not in ['fips', 'zip_code', 'propertylandusedesc']]\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "\n",
    "    # i starts at 0, but plot nos should start at 1\n",
    "    plot_number = i + 1 \n",
    "\n",
    "    # Create subplot.\n",
    "    plt.subplot(1, len(cols), plot_number)\n",
    "\n",
    "    # Title with column name.\n",
    "    plt.title(col)\n",
    "\n",
    "    # Display histogram for column.\n",
    "    zillow[col].hist(bins=5)\n",
    "\n",
    "    # Hide gridlines.\n",
    "    plt.grid(False)\n",
    "    \n",
    "    # turn off scientific notation\n",
    "    #plt.ticklabel_format(useOffset=False)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc29671",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "sns.boxplot(data=zillow.drop(columns=['fips', 'zip_code']))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5df612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns\n",
    "cols = [col for col in zillow.columns if col not in ['fips','zip_code', 'propertylandusedesc']]\n",
    "plt.figure(figsize=(16, 20))\n",
    "for i, col in enumerate(cols):\n",
    "\n",
    "    # i starts at 0, but plot nos should start at 1\n",
    "    plot_number = i + 1 \n",
    "\n",
    "    # Create subplot.\n",
    "    plt.subplot(1, len(cols), plot_number)\n",
    "\n",
    "    # Title with column name.\n",
    "    plt.title(col)\n",
    "\n",
    "    # Display boxplot for column.\n",
    "    sns.boxplot(data=zillow[col])\n",
    "\n",
    "    # Hide gridlines.\n",
    "    plt.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7ed4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adddb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns\n",
    "cols = ['bedrooms', 'bathrooms', 'square_feet', 'home_value', 'taxes']\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "\n",
    "    # i starts at 0, but plot should start at 1\n",
    "    plot_number = i + 1 \n",
    "\n",
    "    # Create subplot.\n",
    "    plt.subplot(1, len(cols), plot_number)\n",
    "\n",
    "    # Title with column name.\n",
    "    plt.title(col)\n",
    "\n",
    "    # Display boxplot for column.\n",
    "    sns.boxplot(data=zillow[[col]])\n",
    "\n",
    "    # Hide gridlines.\n",
    "    plt.grid(False)\n",
    "\n",
    "    # sets proper spacing between plots\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48779c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean up my zillow df\n",
    "def clean_data(df):\n",
    "    '''\n",
    "    This funciton takes in the zillow df and drops observations with Null values\n",
    "    and handles data types returning a df with a basic clean.\n",
    "    '''\n",
    "    df = df.dropna()\n",
    "    df['fips'] = df['fips'].astype(int)\n",
    "    df['zip_code'] = df['zip_code'].astype(int)\n",
    "    df['square_feet'] = df['square_feet'].astype(int)\n",
    "    df['year'] = df['year'].astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8266ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6db4a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow = clean_data(zillow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9140879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a45fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow = clean_data(zillow)\n",
    "print(zillow.shape)\n",
    "zillow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32119960",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcf66ec",
   "metadata": {},
   "source": [
    "### To Do in successve iterations beyond MVP\n",
    "\n",
    "- [ ] (for unchecked checkbox)\n",
    "- [x] (for checked checkbox)\n",
    "\n",
    "\n",
    "- [ ] Add column for range...\n",
    "- [ ] Import additional columns of potential use\n",
    "- [ ] Derive columns from there\n",
    "    - Pool (boolean)\n",
    "    - Condition (bins)\n",
    "    - Calculate age in years\n",
    "    - Bin ages\n",
    "    - Etc.\n",
    "- [ ] Lookup / populate county based on `fips`\n",
    "- [ ] Caculate tax rate percent (`taxes`, `home_value`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0365ce",
   "metadata": {},
   "source": [
    "-[] Add \"inferred single family residential\" code to original SQL query (not necessary, 0 records)\n",
    "\n",
    "- [x] left join on propertylandusetype\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ea1478",
   "metadata": {},
   "source": [
    "Minor, but kind of annoying find out why `[ ]` is not rendering as a checkbox in markdown in VS Code..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ddeafa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5931bd42",
   "metadata": {},
   "source": [
    "## County Data for Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa23fcd8",
   "metadata": {},
   "source": [
    "Fips codes can be found [here](https://www.nrcs.usda.gov/wps/portal/nrcs/detail/national/home/?cid=nrcs143_013697)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de27e397",
   "metadata": {},
   "source": [
    "What fips are used in dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8886781",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.fips.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b60ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "29295+11806+3821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482c8db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d360ae6",
   "metadata": {},
   "source": [
    "### Fips values \n",
    "\n",
    "| fips |   County    | State |\n",
    "| :--: | :---------: | :---: |\n",
    "| 6037 | Los Angeles |  CA   |\n",
    "| 6059 |   Orange    |  CA   |\n",
    "| 6111 |   Ventura   |  CA   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfb0ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a17f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ddd0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to add county based on fips by row\n",
    "\n",
    "def assign_county(row):\n",
    "    if row['fips']==6037:\n",
    "        return 'Los Angeles'\n",
    "    if row['fips']==6059:\n",
    "        return 'Orange'\n",
    "    if row['fips']==6111:\n",
    "        return 'Ventura'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4833e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use function to assign county\n",
    "\n",
    "zillow['county'] = zillow.apply(lambda row: assign_county(row), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a050d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add state columns\n",
    "\n",
    "zillow['state'] = 'CA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a993abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe97b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "## COnvert year into integer (May delete or comment out later since I added this to an earlier function)\n",
    "\n",
    "zillow['year'] = zillow['year'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a083a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confrim year dtype changed\n",
    "\n",
    "zillow.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5ccd1a",
   "metadata": {},
   "source": [
    "### Add county averages to DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccd343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into 3 county dataframes\n",
    "\n",
    "la = zillow[zillow.county=='Los Angeles']\n",
    "oc = zillow[zillow.county=='Orange']\n",
    "ven = zillow[zillow.county=='Ventura']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cfc809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average home price by county\n",
    "\n",
    "la_avg = la.home_value.mean()\n",
    "oc_avg = oc.home_value.mean()\n",
    "ven_avg = ven.home_value.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8070f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add average county price as column\n",
    "\n",
    "def assign_county_avg(row):\n",
    "    if row['fips']==6037:\n",
    "        return la_avg\n",
    "    if row['fips']==6059:\n",
    "        return oc_avg\n",
    "    if row['fips']==6111:\n",
    "        return ven_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f8a1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow['county_avg'] = zillow.apply(lambda row: assign_county_avg(row), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d980339",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.sample(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de17d773",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d02ff2",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "- Will explore more later, but any initial features?\n",
    "- Transform year built to age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5303da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75de9546",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add age column\n",
    "zillow['age'] = date.today().year-zillow.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844779fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm\n",
    "\n",
    "zillow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add2dc02",
   "metadata": {},
   "source": [
    "## Location Data\n",
    "\n",
    "- One-hot encode county so I can pass that to models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1d9a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this to clean module\n",
    "\n",
    "dummy_df = pd.get_dummies(zillow[['county']], drop_first=True)\n",
    "zillow = pd.concat([zillow, dummy_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e319b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.sample(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3217cc",
   "metadata": {},
   "source": [
    "### Add Tax rate column\n",
    "\n",
    "taxes = home_value * tax_rate\n",
    "\n",
    "tax_rate = taxes / home_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3beb2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow['tax_rate']=round(((zillow.taxes/zillow.home_value)*100),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d3b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffba1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.tax_rate.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc38ca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow.tax_rate.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2869f0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41fad0e",
   "metadata": {},
   "source": [
    "## Split data into train, test, validate; Then create separate x/y feature/target dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fc8844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_my_data(df, pct=0.10):\n",
    "    '''\n",
    "    This splits a dataframe into train, validate, and test sets. \n",
    "    df = dataframe to split\n",
    "    pct = size of the test set, 1/2 of size of the validate set\n",
    "    Returns three dataframes (train, validate, test)\n",
    "    '''\n",
    "    train_validate, test = train_test_split(df, test_size=pct, random_state = 123)\n",
    "    train, validate = train_test_split(train_validate, test_size=pct*2, random_state = 123)\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ba0f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = split_my_data(zillow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02259233",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(validate.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bf66ee",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "- Can go ahead and add a baseline, y_hat prediction\n",
    "- Will use the training median as baseline, less sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b88e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Baseline\n",
    "\n",
    "baseline = train.home_value.median()\n",
    "baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d760589",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['baseline'] = baseline\n",
    "validate['baseline'] = baseline\n",
    "test['baseline'] = baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd93ed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7956c459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into x / y | features / target\n",
    "\n",
    "# Setup X and y\n",
    "X_train = train.drop(columns='home_value')\n",
    "y_train = train.home_value\n",
    "\n",
    "X_validate = validate.drop(columns='home_value')\n",
    "y_validate = validate.home_value\n",
    "\n",
    "X_test = test.drop(columns='home_value')\n",
    "y_test = test.home_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902bacd0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37040182",
   "metadata": {},
   "source": [
    "## Data has been aquired and cleaned, now scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9133e3",
   "metadata": {},
   "source": [
    "Since even our cleaned data has a fair number of outliers still, I will use the robust scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304e7e0d",
   "metadata": {},
   "source": [
    "### Beginning exploration\n",
    "\n",
    "- Examine pairwaise relationships\n",
    "    - Crosstabs\n",
    "    - Corr plots\n",
    "    - Pair plots\n",
    "    - Etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8352cea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84187b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b64d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bb93fc",
   "metadata": {},
   "source": [
    "**NOTE:** I originally scaled `fips` and `zip_code` because I wanted to pass more granular location data to my model. However, using a scaled value imploes a numberic relationship between the values (90210 being 100 more somethings from 90110, which is not the case). So, I am eliminating them from the scaler.\n",
    "\n",
    "- To get in more granular location data, in a sucessive iteration I will one-hot encode the counties to pass along to the model\n",
    "\n",
    "- To dive in finer:\n",
    "    - Pull lat lon from the original database and see how many nulls there are\n",
    "    - Convert zip to lat lon\n",
    "    - Use an unsupervised clustering algorithm to create k neighborhoods\n",
    "    - Explore approppriate k for number of obeservations; 3-6?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5261f227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit scaler to training data\n",
    "\n",
    "scaler = sklearn.preprocessing.RobustScaler()\n",
    "\n",
    "columns = ['bedrooms', 'bathrooms', 'square_feet',  'age']\n",
    "\n",
    "scaler.fit(X_train[columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7224e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply scaler to all data\n",
    "\n",
    "new_column_names = [c + '_scaled' for c in columns]\n",
    "\n",
    "X_train = pd.concat([X_train, pd.DataFrame(scaler.transform(X_train[columns]), columns=new_column_names, index = train.index),], axis=1)\n",
    "\n",
    "X_validate = pd.concat([X_validate, pd.DataFrame(scaler.transform(X_validate[columns]), columns=new_column_names, index = validate.index),], axis=1)\n",
    "\n",
    "X_test = pd.concat([X_test, pd.DataFrame(scaler.transform(X_test[columns]), columns=new_column_names, index = test.index),], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb181ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce635c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf80987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vizualize scaler\n",
    "\n",
    "plt.figure(figsize=(13, 6))\n",
    "plt.subplot(121)\n",
    "plt.hist(X_train.square_feet, bins=25, ec='black')\n",
    "plt.title('Original')\n",
    "plt.subplot(122)\n",
    "plt.hist(X_train.square_feet_scaled, bins=25, ec='black')\n",
    "plt.title('Scaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802e1b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vizualize scaler\n",
    "\n",
    "plt.figure(figsize=(13, 6))\n",
    "plt.subplot(121)\n",
    "plt.hist(X_train.bedrooms, bins=25, ec='black')\n",
    "plt.title('Original')\n",
    "plt.subplot(122)\n",
    "plt.hist(X_train.bedrooms_scaled, bins=25, ec='black')\n",
    "plt.title('Scaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571f7b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vizualize scaler\n",
    "\n",
    "plt.figure(figsize=(13, 6))\n",
    "plt.subplot(121)\n",
    "plt.hist(X_train.bathrooms, bins=25, ec='black')\n",
    "plt.title('Original')\n",
    "plt.subplot(122)\n",
    "plt.hist(X_train.bathrooms_scaled, bins=25, ec='black')\n",
    "plt.title('Scaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3431b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vizualize scaler\n",
    "\n",
    "plt.figure(figsize=(13, 6))\n",
    "plt.subplot(121)\n",
    "plt.hist(X_train.age, bins=25, ec='black')\n",
    "plt.title('Original')\n",
    "plt.subplot(122)\n",
    "plt.hist(X_train.age_scaled, bins=25, ec='black')\n",
    "plt.title('Scaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c83274",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cc5743",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b66f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Combine all of the above steps into function(s)#################\n",
    "\n",
    "#################### IMPORT LIBRARIES #################\n",
    "\n",
    "# Basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "#Vizualization Tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Modeling Tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "import sklearn.preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Custim functions\n",
    "from env import host, user, password #Database credentials\n",
    "import zillo_wrangle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################ PULL DATA FROM DB ############## \n",
    "\n",
    "def get_db_url(db_name):\n",
    "    return f\"mysql+pymysql://{user}:{password}@{host}/{db_name}\"\n",
    "\n",
    "\n",
    "def get_data_from_sql():\n",
    "    query = \"\"\"\n",
    "    SELECT bedroomcnt as bedrooms, \n",
    "       bathroomcnt as bathrooms,\n",
    "       calculatedfinishedsquarefeet as square_feet,\n",
    "       yearbuilt as year,\n",
    "       taxamount as taxes,\n",
    "       taxvaluedollarcnt as home_value,\n",
    "       fips as fips,\n",
    "       regionidzip as zip_code\n",
    "    FROM predictions_2017\n",
    "     JOIN properties_2017 USING(parcelid)\n",
    "    WHERE (transactiondate >= '2017-01-01' AND transactiondate <= '2017-12-31') \n",
    "        AND propertylandusetypeid = '261'\n",
    "        AND bedroomcnt > 0\n",
    "        AND bathroomcnt > 0\n",
    "        AND calculatedfinishedsquarefeet > 0 \n",
    "        AND taxamount > 0\n",
    "        AND taxvaluedollarcnt > 0\n",
    "        AND fips > 0\n",
    "    ORDER BY fips;\n",
    "    \"\"\"\n",
    "    df = pd.read_sql(query, get_db_url('zillow'))\n",
    "    return df\n",
    "\n",
    "\n",
    "################ REMOVE OUTLIERS #################\n",
    "\n",
    "def remove_outliers(df, k, col_list):\n",
    "    ''' remove outliers from a list of columns in a dataframe \n",
    "        and return that dataframe\n",
    "    '''\n",
    "    \n",
    "    for col in col_list:\n",
    "\n",
    "        q1, q3 = df[col].quantile([.25, .75])  # get quartiles\n",
    "        \n",
    "        iqr = q3 - q1   # calculate interquartile range\n",
    "        \n",
    "        upper_bound = q3 + k * iqr   # get upper bound\n",
    "        lower_bound = q1 - k * iqr   # get lower bound\n",
    "\n",
    "        # return dataframe without outliers\n",
    "        \n",
    "        df = df[(df[col] > lower_bound) & (df[col] < upper_bound)]\n",
    "        \n",
    "    return df\n",
    "\n",
    "######## Clean Data ###########\n",
    "\n",
    "def clean_data(df):\n",
    "    '''\n",
    "    This funciton takes in the zillow df and drops  Null values reassigns some dtypes.\n",
    "    '''\n",
    "    df = df.dropna()\n",
    "    df['fips'] = df['fips'].astype(int)\n",
    "    df['zip_code'] = df['zip_code'].astype(int)\n",
    "    df['square_feet'] = df['square_feet'].astype('int')\n",
    "    df['year'] = df['year'].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "######### ADD COUNTY AND STATE COLUMNS #######\n",
    "\n",
    "def assign_county(row):\n",
    "    if row['fips']==6037:\n",
    "        return 'Los Angeles'\n",
    "    if row['fips']==6059:\n",
    "        return 'Orange'\n",
    "    if row['fips']==6111:\n",
    "        return 'Ventura'\n",
    "\n",
    "######## Feature engineering ########\n",
    "\n",
    "def engineer(zillow):\n",
    "    zillow['county'] = zillow.apply(lambda row: assign_county(row), axis =1) #Add counties\n",
    "    zillow['state'] = 'CA' #Add state\n",
    "    zillow['age'] = date.today().year-zillow.year # Add age\n",
    "    dummy_df = pd.get_dummies(zillow[['county']], drop_first=True)\n",
    "    zillow = pd.concat([zillow, dummy_df], axis=1)\n",
    "    return zillow\n",
    "\n",
    "###### ADD COUNTY AVERAGE #############\n",
    "\n",
    "def county_avg(zillow):\n",
    "    la = zillow[zillow.county=='Los Angeles']\n",
    "    oc = zillow[zillow.county=='Orange']\n",
    "    ven = zillow[zillow.county=='Ventura']\n",
    "\n",
    "    la_avg = la.home_value.mean()\n",
    "    oc_avg = oc.home_value.mean()\n",
    "    ven_avg = ven.home_value.mean()\n",
    "\n",
    "    def assign_county_avg(row):\n",
    "        if row['fips']==6037:\n",
    "            return la_avg\n",
    "        if row['fips']==6059:\n",
    "            return oc_avg\n",
    "        if row['fips']==6111:\n",
    "            return ven_avg\n",
    "\n",
    "    zillow['county_avg'] = zillow.apply(lambda row: assign_county_avg(row), axis =1)\n",
    "\n",
    "    return zillow\n",
    "\n",
    "########## TRAIN VALIDATE TEST SPLIT #########\n",
    "\n",
    "def split_my_data(df, pct=0.10):\n",
    "    '''\n",
    "    This splits a dataframe into train, validate, and test sets. \n",
    "    df = dataframe to split\n",
    "    pct = size of the test set, 1/2 of size of the validate set\n",
    "    Returns three dataframes (train, validate, test)\n",
    "    '''\n",
    "    train_validate, test = train_test_split(df, test_size=pct, random_state = 123)\n",
    "    train, validate = train_test_split(train_validate, test_size=pct*2, random_state = 123)\n",
    "    return train, validate, test\n",
    "\n",
    "########## ADD BASELINE #########\n",
    "\n",
    "def add_baseline(train, validate, test):\n",
    "    baseline = train.home_value.median()\n",
    "    train['baseline'] = baseline\n",
    "    validate['baseline'] = baseline\n",
    "    test['baseline'] = baseline\n",
    "    return train, validate, test\n",
    "\n",
    "######## SPLIT IN TO X /y features / target ########\n",
    "\n",
    "def split_xy(train, validate, test):\n",
    "    X_train = train.drop(columns='home_value')\n",
    "    y_train = train.home_value\n",
    "\n",
    "    X_validate = validate.drop(columns='home_value')\n",
    "    y_validate = validate.home_value\n",
    "\n",
    "    X_test = test.drop(columns='home_value')\n",
    "    y_test = test.home_value\n",
    "\n",
    "    return X_train, y_train, X_validate, y_validate, X_test, y_test\n",
    "\n",
    "############## Robust Scale ###############\n",
    "\n",
    "def scale(X_train, X_validate, X_test, train, validate, test):\n",
    "    scaler = sklearn.preprocessing.RobustScaler()\n",
    "\n",
    "    columns = ['bedrooms', 'bathrooms', 'square_feet', 'fips', 'age', 'zip_code']\n",
    "    \n",
    "    scaler.fit(X_train[columns])\n",
    "\n",
    "    new_column_names = [c + '_scaled' for c in columns]\n",
    "\n",
    "    X_train = pd.concat([X_train, pd.DataFrame(scaler.transform(X_train[columns]), columns=new_column_names, index = train.index),], axis=1)\n",
    "\n",
    "    X_validate = pd.concat([X_validate, pd.DataFrame(scaler.transform(X_validate[columns]), columns=new_column_names, index = validate.index),], axis=1)\n",
    "\n",
    "    X_test = pd.concat([X_test, pd.DataFrame(scaler.transform(X_test[columns]), columns=new_column_names, index = test.index),], axis=1)\n",
    "    \n",
    "    return X_train, X_validate, X_test\n",
    "\n",
    "######### CALL ALL FUNCTIONS TOGETHER #######\n",
    "\n",
    "def wrangle():\n",
    "    zillow = get_data_from_sql()\n",
    "    zillow = remove_outliers(zillow, 1.5, ['bedrooms', 'bathrooms', 'square_feet', 'taxes', 'home_value'])\n",
    "    zillow = clean_data(zillow) #Drop NAs and change dtypes\n",
    "    zillow = engineer(zillow)\n",
    "    zillow = county_avg(zillow)\n",
    "    train, validate, test = split_my_data(zillow)\n",
    "    train, validate, test = add_baseline(train, validate, test)\n",
    "    X_train, y_train, X_validate, y_validate, X_test, y_test = split_xy(train, validate, test)\n",
    "    X_train, X_validate, X_test = scale(X_train, X_validate, X_test, train, validate, test)\n",
    "    return X_train, y_train, X_validate, y_validate, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ea62f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97f61b0",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7de5a5e",
   "metadata": {},
   "source": [
    "### Initial questions:\n",
    "\n",
    "1. What are drivers of tax value?\n",
    "2. What leads to lower tax values?\n",
    "3. What factors do not impact tax value?\n",
    "4. Is there a difference in average price by county\n",
    "5. Are there any ways to combine the current data into interesting engineered features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f97a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e84cad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc34de80",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89b5fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at distribution of all numeric columns in df\n",
    "\n",
    "df.hist(grid=False, figsize=(16,12));\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd352e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63699248",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['bedrooms', 'bathrooms', 'square_feet', 'age', 'home_value']\n",
    "sns.heatmap(df[columns].corr(), cmap='Blues', annot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83784605",
   "metadata": {},
   "source": [
    "### Initial impressions\n",
    "\n",
    "- Looking at factors that impact home value, Square footage seems to be the highest driver, followed by bathrooms, then bedrooms\n",
    "\n",
    "- There is a negative correlation with age\n",
    "\n",
    "- Bedrooms matter least out of these sparse columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb33626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a45ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_exploration(df, x_string, y_string):\n",
    "    r, p = stats.pearsonr(df[x_string], df[y_string])\n",
    "    ax= sns.regplot(x=x_string, y=y_string, data=df, line_kws={\"color\": \"red\"})\n",
    "    plt.title(f\"{x_string}'s Relationship with {y_string}\")\n",
    "    print(f'The p-value is: {p}. There is {round(p,3)}% chance that we see these results by chance.')\n",
    "    print(f'r = {round(r, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bdca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_exploration(df, 'square_feet', 'home_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_exploration(df, 'bathrooms', 'home_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384fb172",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_exploration(df, 'bedrooms', 'home_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0f9ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_exploration(df, 'age', 'home_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaebc7a3",
   "metadata": {},
   "source": [
    "- [x]Add lines of best fit to scatterplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3544e217",
   "metadata": {},
   "source": [
    "Statistical tests confirm, strongest correlation with area, followed by bathrooms, then bedrooms with age being negatively corrolated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fada50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='square_feet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf247ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, y='bedrooms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd3ea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, y='bathrooms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b127823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4f4d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6cf8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='county')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0ed9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['zip_code']=df['zip_code'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde5f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='zip_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccda47e7",
   "metadata": {},
   "source": [
    "weird scaling issues - play arond more if you really care to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea8a85",
   "metadata": {},
   "source": [
    "Is there a difference in home prices by county?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d58ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a separate dataframe for each county\n",
    "\n",
    "la = train[train.county=='Los Angeles'].home_value\n",
    "oc = train[train.county=='Orange'].home_value\n",
    "ven = train[train.county=='Ventura'].home_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd824fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualize prices by county\n",
    "\n",
    "ax = sns.boxplot(x=train.county, y=train.home_value)\n",
    "#ax = sns.swarmplot(x=train.county, y=train.home_value, color ='.25')\n",
    "plt.title('Figure 5: Average value by County')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b755351e",
   "metadata": {},
   "source": [
    "- Perform Kruskal-Wallis H-test to see if there is a difference by county.\n",
    "\n",
    "- Using a nonparametric test since the underlying distributions are not normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864a05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.kruskal(la, oc, ven)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5adc733",
   "metadata": {},
   "source": [
    "- There are differences by county. \n",
    "\n",
    "- Post-hoc, pairwaise analysis can determine which samples are different in which direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cca7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is LA lower than OC\n",
    "stats.mannwhitneyu(la, oc, alternative ='less')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc73ca5d",
   "metadata": {},
   "source": [
    "- Los Angeles County prices are lower, on average, than Orange County "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49463e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is LA lower than Ventura\n",
    "\n",
    "stats.mannwhitneyu(la, ven, alternative ='less')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec106254",
   "metadata": {},
   "source": [
    "- Los Angeles County prices are lower, on average, than Ventura County "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d82057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is Ventura Lower than Orange\n",
    "\n",
    "stats.mannwhitneyu(ven, oc, alternative ='less')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3f642f",
   "metadata": {},
   "source": [
    "Ventura is not statistically lower than orange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58b4c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there a difference between Ventura and orange\n",
    "\n",
    "stats.mannwhitneyu(ven, oc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cfc298",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.kruskal(oc, ven)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9bd162",
   "metadata": {},
   "source": [
    "There is no statistically significant difference between the mean home values in Orange and Ventura Counties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05be543",
   "metadata": {},
   "source": [
    "### Ideas for final figures:\n",
    "\n",
    "- Figure 1 - Distributions: Panels with distributions of all target varibles\n",
    "- Figure 2 - Drivers: panels with correlations and R/p labelled\n",
    "- Figure 3 - Redce (age)\n",
    "- Figure 4 - Don't matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac79b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='home_value', hue='bedrooms', palette='husl', alpha = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908145b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='home_value', hue='bathrooms', palette='husl', alpha = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08655277",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d873bcc9",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Data is now clean split, and scaled and we have a baseline and a feel for meaningful drivers. Proceed with creating MVP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72058709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Import ols\n",
    "\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f0b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "\n",
    "ols_model = ols(formula='home_value ~ bedrooms + bathrooms + square_feet', data=train).fit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7fcb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "\n",
    "ols_yhat = ols_model.predict(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea6ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['mvp_prdictions']=ols_yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5986b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebab724",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a6bd74",
   "metadata": {},
   "source": [
    "### Evaluate model\n",
    "\n",
    "- RMSE and R^2 will be primary evaluation metrics\n",
    "- Make a dataframe with actual values, baseline predictions, and model predictions\n",
    "- Calculate RMSE\n",
    "- Calculate R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b781e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  DataFrame for evaluating model \n",
    "\n",
    "ols_eval = y_train.copy()\n",
    "validate_eval = y_validate.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ddf8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_eval = pd.DataFrame(ols_eval)\n",
    "validate_eval = pd.DataFrame(validate_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1238dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_eval.rename(columns={'home_value': 'actual'}, inplace=True)\n",
    "validate_eval.rename(columns={'home_value': 'actual'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3c8506",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6534c2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add baseline - median home value\n",
    "\n",
    "ols_eval['baseline_yhat'] = ols_eval['actual'].median()\n",
    "\n",
    "validate_eval['baseline_yhat'] = ols_eval['actual'].median()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82af90e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model prediction\n",
    "\n",
    "ols_eval['ols_yhat'] = ols_model.predict(X_train)\n",
    "validate_eval['ols_yhat'] = ols_model.predict(X_validate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2b06c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9226d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and Add Residuals Column for Plotting\n",
    "\n",
    "ols_eval['residuals'] = ols_eval.ols_yhat - ols_eval.actual\n",
    "validate_eval['residuals'] = validate_eval.ols_yhat - validate_eval.actual\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401807ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab42d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Compute the RMSE  and R2 for  ols model and baseline \n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from math import sqrt\n",
    "\n",
    "baseline_RMSE = round(sqrt(mean_squared_error(ols_eval.actual, ols_eval.baseline_yhat)))\n",
    "ols_RMSE = round(sqrt(mean_squared_error(ols_eval.actual, ols_eval.ols_yhat)))\n",
    "pct_change=round(((ols_RMSE-baseline_RMSE)/baseline_RMSE)*100, 2)\n",
    "rmse_validate = round(sqrt(mean_squared_error(validate_eval.actual, validate_eval.ols_yhat)))\n",
    "baseline_r2 = round(r2_score(ols_eval.actual, ols_eval.baseline_yhat), 2)\n",
    "ols_train_r2 = round(r2_score(ols_eval.actual, ols_eval.ols_yhat), 2)\n",
    "ols_validate_r2 = round(r2_score(validate_eval.actual, validate_eval.ols_yhat), 2)\n",
    "\n",
    "print(f'My model has value: {ols_RMSE < baseline_RMSE}')\n",
    "print()\n",
    "print(f'Baseline RMSE: {baseline_RMSE}')\n",
    "print(f'My model train RMSE: {ols_RMSE}')\n",
    "print(f'My model validate RMSE: {rmse_validate}')\n",
    "print(f'RMSE difference baseline to model: {baseline_RMSE- ols_RMSE}')\n",
    "print(f'RMSE difference train to validate: {ols_RMSE- rmse_validate}')\n",
    "print(f'RMSE improvement: {pct_change}%')\n",
    "print()\n",
    "print(f'Baseline R2: {baseline_r2}')\n",
    "print(f'Model train  R2: {ols_train_r2}')\n",
    "print(f'Model Validate R2: {ols_validate_r2}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c1466",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49fb76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at risiduals distribution\n",
    "\n",
    "plt.hist(np.log(ols_eval.residuals));\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cabe7c",
   "metadata": {},
   "source": [
    "Definitley a skew to the residuals - not centered around 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb75229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at Predictions vs Residuals\n",
    "\n",
    "plt.scatter(ols_eval.ols_yhat, ols_eval.residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae34c9a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0c36d7",
   "metadata": {},
   "source": [
    "Next model: same as before, but add age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c029b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d3ad87",
   "metadata": {},
   "source": [
    "## OLS2: add age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0f3a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589ea780",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols2_model = ols(formula='home_value ~ bedrooms + bathrooms + square_feet + age',  data=train).fit() #Create model\n",
    "ols2_yhat = ols2_model.predict(X_train) # Make predictions\n",
    "X_train['model2']=ols2_yhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e7743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to eval\n",
    "ols_eval['ols2_yhat'] = ols2_model.predict(X_train)\n",
    "validate_eval['ols2_yhat'] = ols2_model.predict(X_validate)\n",
    "ols_eval['ols2_residuals'] = ols_eval.ols2_yhat - ols_eval.actual\n",
    "validate_eval['ols2_residuals'] = validate_eval.ols2_yhat - validate_eval.actual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedac0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_RMSE = round(sqrt(mean_squared_error(ols_eval.actual, ols_eval.baseline_yhat)))\n",
    "ols_RMSE = round(sqrt(mean_squared_error(ols_eval.actual, ols_eval.ols_yhat)))\n",
    "ols2_RMSE = round(sqrt(mean_squared_error(ols_eval.actual, ols_eval.ols2_yhat)))\n",
    "pct_change_baseline=round(((ols_RMSE-baseline_RMSE)/baseline_RMSE)*100, 2)\n",
    "pct_change_last_model=round(((ols2_RMSE-ols_RMSE)/ols_RMSE)*100, 2)\n",
    "rmse_validate = round(sqrt(mean_squared_error(validate_eval.actual, validate_eval.ols2_yhat)))\n",
    "baseline_r2 = round(r2_score(ols_eval.actual, ols_eval.baseline_yhat), 2)\n",
    "ols2_train_r2 = round(r2_score(ols_eval.actual, ols_eval.ols2_yhat), 2)\n",
    "ols2_validate_r2 = round(r2_score(validate_eval.actual, validate_eval.ols2_yhat), 2)\n",
    "\n",
    "\n",
    "print(f'My model has value: {ols_RMSE < baseline_RMSE}')\n",
    "print(f'My model beats previous model: {ols2_RMSE < ols_RMSE}')\n",
    "print()\n",
    "print(f'Baseline RMSE: {baseline_RMSE}')\n",
    "print(f'Model 1 RMSE: {ols_RMSE}')\n",
    "print(f'Currennt model train RMSE: {ols2_RMSE}')\n",
    "print(f'Currennt model validate RMSE: {rmse_validate}')\n",
    "print()\n",
    "print(f'Current model RMSE difference from baseline: {baseline_RMSE- ols_RMSE}')\n",
    "print(f'RMSE difference train to validate: {ols2_RMSE- rmse_validate}')\n",
    "print(f'Current model baseline RMSE improvement: {pct_change_baseline}%')\n",
    "print(f'Current model RMSE improvement from last model: {pct_change_last_model}%')\n",
    "print()\n",
    "print(f'Baseline R2: {baseline_r2}')\n",
    "print(f'Model train  R2: {ols2_train_r2}')\n",
    "print(f'Model Validate R2: {ols2_validate_r2}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63078066",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ba3317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at risiduals distribution\n",
    "\n",
    "plt.hist(np.log(ols_eval.ols2_residuals));\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aaad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at Predictions vs Residuals\n",
    "\n",
    "plt.scatter(ols_eval.ols_yhat, ols_eval.ols2_residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92d9cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols2_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550fb339",
   "metadata": {},
   "source": [
    "### Model 3: add county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875a1011",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aab687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols3_model = ols(formula='home_value ~ bedrooms + bathrooms + square_feet + age + county_Orange + county_Ventura',  data=train).fit() #Create model\n",
    "ols3_yhat = ols3_model.predict(X_train) # Make predictions\n",
    "X_train['model3']=ols3_yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ded39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b99b801",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_eval['ols3_yhat'] = ols3_model.predict(X_train)\n",
    "ols_eval['ols3_residuals'] = ols_eval.ols3_yhat - ols_eval.actual\n",
    "validate_eval['ols3_yhat'] = ols3_model.predict(X_validate)\n",
    "validate_eval['ols3_residuals'] = validate_eval.ols3_yhat - validate_eval.actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60163ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7841c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb661a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_RMSE = round(sqrt(mean_squared_error(ols_eval.actual, ols_eval.baseline_yhat)))\n",
    "ols_RMSE = round(sqrt(mean_squared_error(ols_eval.actual, ols_eval.ols_yhat)))\n",
    "ols2_RMSE = round(sqrt(mean_squared_error(ols_eval.actual, ols_eval.ols2_yhat)))\n",
    "ols3_RMSE = round(sqrt(mean_squared_error(ols_eval.actual, ols_eval.ols3_yhat)))\n",
    "\n",
    "pct_change_baseline=round(((ols3_RMSE-baseline_RMSE)/baseline_RMSE)*100, 2)\n",
    "pct_change_last_model=round(((ols3_RMSE-ols2_RMSE)/ols2_RMSE)*100, 2)\n",
    "\n",
    "ols3_train_r2 = round(r2_score(ols_eval.actual, ols_eval.ols3_yhat), 2)\n",
    "ols3_validate_r2 = round(r2_score(validate_eval.actual, validate_eval.ols3_yhat), 2)\n",
    "\n",
    "print(f'My model has value: {ols3_RMSE < baseline_RMSE}')\n",
    "print(f'My model beats previous model: {ols3_RMSE < ols2_RMSE}')\n",
    "print()\n",
    "print(f'Baseline RMSE: {baseline_RMSE}')\n",
    "print(f'Model 1 RMSE: {ols_RMSE}')\n",
    "print(f'Model 2 RMSE: {ols2_RMSE}')\n",
    "print(f'Current model RMSE: {ols3_RMSE}')\n",
    "print()\n",
    "print(f'Current model RMSE difference from baseline: {baseline_RMSE- ols3_RMSE}')\n",
    "print(f'Current model baseline RMSE improvement: {pct_change_baseline}%')\n",
    "print(f'Current model RMSE improvement from last model: {pct_change_last_model}%')\n",
    "print()\n",
    "print(f'Baseline R2: {baseline_r2}')\n",
    "print(f'Current Model train  R2: {ols3_train_r2}')\n",
    "print(f'Current Model Validate R2: {ols3_validate_r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e121cb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at risiduals distribution\n",
    "\n",
    "plt.hist(np.log(ols_eval.ols3_residuals));\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at Predictions vs Residuals\n",
    "\n",
    "plt.scatter(ols_eval.ols_yhat, ols_eval.ols3_residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f289038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols3_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60687f67",
   "metadata": {},
   "source": [
    "RMSD went down firther and R^2 increased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cf10dd",
   "metadata": {},
   "source": [
    "[] Add RMSE for validate on model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f0e97b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3c3c1d",
   "metadata": {},
   "source": [
    "### Takeaways / Feature Engineering\n",
    "\n",
    "- MVP model beats baseline\n",
    "- Now, try to beat model\n",
    "- How? Feature engineering\n",
    "- Potential features:\n",
    "    - Incorporate location:\n",
    "        - One-hot encoded counties for now\n",
    "        - Future iterations: Cluster lat/lon for more granular location detail\n",
    "    - Add boolean columns:\n",
    "        - Garage / no garage\n",
    "        - Pool / no pool\n",
    "        - Etc.\n",
    "    - Room score: some kind of single value incorporating both the bedrooms and bathrooms\n",
    "        - Maybe multiplied / weighted by their correlation coefficients\n",
    "    - Stratify columns:\n",
    "        - Age: historical, new constuction, in between"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37b4d30",
   "metadata": {},
   "source": [
    "- Build new, select * query to bring in more fields to use for engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9aad6a",
   "metadata": {},
   "source": [
    "### EVALUATION VIZUALS\n",
    "\n",
    "- [] Better actual vs predicted plots\n",
    "    - For a small enough sample of values you can see them\n",
    "- []Residual plots\n",
    "- [] Histograms of actual vs predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f008755c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c5e0ca",
   "metadata": {},
   "source": [
    "### LASSO / LARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037c1ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3428810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "lars = LassoLars(alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5142d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70589110",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns =['bedrooms_scaled', 'bathrooms_scaled','square_feet_scaled', 'age_scaled', 'county_Orange', 'county_Ventura']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e6718",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2657680",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f254ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lasso_lars = lars.fit(X_train[columns], y_train.home_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c61c2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train['lars_predict']=lars.predict(X_train[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92166d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_train = sqrt(mean_squared_error(y_train.home_value, y_train.lars_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a641d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359142ca",
   "metadata": {},
   "source": [
    "Basely worse than model 3 above by RMSE, check R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff09656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d106841",
   "metadata": {},
   "outputs": [],
   "source": [
    "lars_r2 =r2_score(y_train.home_value, y_train.lars_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1048c51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lars_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d7b1c7",
   "metadata": {},
   "source": [
    "# TO DO \n",
    "\n",
    "- [X] Add R2 to printed summaries\n",
    "- [] Hard Code Datafram of models with rmse and r2 for easy, side-by-side comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb384cb3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cde0b1",
   "metadata": {},
   "source": [
    "Using a separate wrangle file to feature engineer a more robust model beyond my MVP without breaking anything in that,\n",
    "\n",
    "### Plan:\n",
    "1. Use the same query - change only scalers\n",
    "    -  Linear & Standard\n",
    "    - See if it improves model\n",
    "2. Import all the fields from the database, clean them up, and use K-best for modeling\n",
    "    - Rename columns\n",
    "    - Drop Columns\n",
    "    - Engineer columns\n",
    "    - Encode columns\n",
    "    - Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb967c34",
   "metadata": {},
   "source": [
    "## Min-Max scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b428ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import z_wrangle2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834d2003",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mm, X_train_mm, y_train_mm, X_validate_mm, y_validate_mm, X_test_mm, y_test_mm = z_wrangle2.wrangle()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a05325",
   "metadata": {},
   "source": [
    "Test this on current best performing model and compare to current performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae8521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make model and predictions\n",
    "ols3_model = ols(formula='home_value ~ bedrooms + bathrooms + square_feet + age + county_Orange + county_Ventura',  data=train).fit() #Create model\n",
    "ols3_yhat_mm = ols3_model.predict(X_train_mm) # Make predictions\n",
    "X_train['model3_mm']=ols3_yhat\n",
    "ols_eval['ols3_yhat_mm'] = ols3_model.predict(X_train_mm)\n",
    "ols_eval['ols3_residuals_mm'] = ols_eval.ols3_yhat_mm - ols_eval.actual\n",
    "validate_eval['ols3_yhat_mm'] = ols3_model.predict(X_validate_mm)\n",
    "validate_eval['ols3_residuals_mm'] = validate_eval.ols3_yhat_mm - validate_eval.actual\n",
    "# Calculate evaluation metrics\n",
    "baseline_RMSE = round(sqrt(mean_squared_error(ols_eval.actual, ols_eval.baseline_yhat)))\n",
    "ols_RMSE = round(sqrt(mean_squared_error(ols_eval.actual, ols_eval.ols_yhat)))\n",
    "ols2_RMSE = round(sqrt(mean_squared_error(ols_eval.actual, ols_eval.ols2_yhat)))\n",
    "ols3_RMSE = round(sqrt(mean_squared_error(ols_eval.actual, ols_eval.ols3_yhat)))\n",
    "ols3_RMSE_mm = round(sqrt(mean_squared_error(ols_eval.actual, ols_eval.ols3_yhat_mm)))\n",
    "\n",
    "\n",
    "pct_change_baseline=round(((ols3_RMSE_mm-baseline_RMSE)/baseline_RMSE)*100, 2)\n",
    "pct_change_last_model=round(((ols3_RMSE_mm-ols3_RMSE)/ols3_RMSE)*100, 2)\n",
    "\n",
    "ols3_train_r2_mm = round(r2_score(ols_eval.actual, ols_eval.ols3_yhat_mm), 2)\n",
    "ols3_validate_r2_mm = round(r2_score(validate_eval.actual, validate_eval.ols3_yhat_mm), 2)\n",
    "# Display findings\n",
    "print(f'My model has value: {ols3_RMSE_mm < baseline_RMSE}')\n",
    "print(f'My model beats previous model: {ols3_RMSE_mm < ols3_RMSE}')\n",
    "print()\n",
    "print(f'Baseline RMSE: {baseline_RMSE}')\n",
    "print(f'Model 1 RMSE: {ols_RMSE}')\n",
    "print(f'Model 2 RMSE: {ols2_RMSE}')\n",
    "print(f'Model 3 RMSE: {ols3_RMSE}')\n",
    "print(f'Current model RMSE: {ols3_RMSE_mm}')\n",
    "print()\n",
    "print(f'Current model RMSE difference from baseline: {baseline_RMSE- ols3_RMSE_mm}')\n",
    "print(f'Current model baseline RMSE improvement: {pct_change_baseline}%')\n",
    "print(f'Current model RMSE improvement from last model: {pct_change_last_model}%')\n",
    "print()\n",
    "print(f'Baseline R2: {baseline_r2}')\n",
    "print(f'Current Model train  R2: {ols3_train_r2}')\n",
    "print(f'Current Model Validate R2: {ols3_validate_r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e7643c",
   "metadata": {},
   "source": [
    "The robust scaler and min max scaler produced the same results. \n",
    "\n",
    "[ ] If time, Try again with standard scaler. For now, go to full database pull snd feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769a821c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af557a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a select * from the database and engineer features from there\n",
    "import z_wrangle3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58274e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train, X_train, y_train, X_validate, y_validate, X_test, y_test = z_wrangle3.wrangle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3558e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ab905",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4e76c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
